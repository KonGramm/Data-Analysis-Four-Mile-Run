main = "HR.Rest by Training Effect Category",
xlab = "Training Effect Category", ylab = "HR.Rest")
# Plot 4: Box Plot of HR.Change1 by Training Effect Category
boxplot(HR.Change1 ~ Train.Effect.Cat.Short, data = FourMileRun, outline = TRUE, col = colors[[4]],
main = "HR.Change1 by Training Effect Category",
xlab = "Training Effect Category", ylab = "HR.Change1")
# Plot 5: Box Plot of HR.Change2 by Training Effect Category
boxplot(HR.Change2 ~ Train.Effect.Cat.Short, data = FourMileRun, outline = TRUE, col = colors[[5]],
main = "HR.Change2 by Training Effect Category",
xlab = "Training Effect Category", ylab = "HR.Change2")
par(mfrow = c(1, 1))
# Create a boxplot of the Calories Burned in each Training Effect Category
colors <- c("skyblue", "lightgreen", "coral2", "lightgoldenrodyellow", "lightsalmon")
boxplot(Calories.Burned  ~ Train.Effect.Cat, data = FourMileRun, outline = TRUE, col = colors,cex.axis = 0.8,main = "Box Plot of Calories Burned by Training Effect Category", xlab = "Training Effect Category", ylab = "Calories Burned")
# Create a boxplot of the Time In Minutes in each Training Effect Category
colors <- c("skyblue", "lightgreen", "darkseagreen2", "lightgoldenrodyellow", "lightsalmon")
boxplot(Time.In.Minutes  ~ Train.Effect.Cat, data = FourMileRun, outline = TRUE, col = colors,cex.axis = 0.8,main = "Box Plot of ime In Minutes by Training Effect Category", xlab = "Training Effect Category", ylab = "ime In Minutes")
# Create a boxplot of the Pace In Minutes in each Training Effect Category
colors <- c("skyblue", "lightgreen", "deeppink3", "lightgoldenrodyellow", "lightsalmon")
boxplot(Pace.In.Minutes~ Train.Effect.Cat, data = FourMileRun, outline = TRUE, col = colors,cex.axis = 0.8,main = "Box Plot of Pace In Minutes  by Training Effect Category", xlab = "Training Effect Category", ylab = "Pace In Minutes")
#Boxplot of the Avg Speed in each Training Effect Category
colors <- c("skyblue", "lightgreen", "azure4", "lightgoldenrodyellow", "lightsalmon")
boxplot(Avg.Speed  ~ Train.Effect.Cat, data = FourMileRun, outline = TRUE, col = colors,cex.axis = 0.8,main = "Box Plot of Average Speed by Training Effect Category", xlab = "Training Effect Category", ylab = "Average Speed")
#Boxplot of the Max Speed in each Training Effect Category
colors <- c("skyblue", "lightgreen", "forestgreen", "lightgoldenrodyellow", "lightsalmon")
boxplot(Max.Speed  ~ Train.Effect.Cat, data = FourMileRun, outline = TRUE, col = colors,cex.axis = 0.8,main = "Box Plot of Max Speed by Training Effect Category", xlab = "Training Effect Category", ylab = "Max Speed")
#Boxplot of the Heart Rate 1 Minute After Run in each Training Effect Category
colors <- c("skyblue", "lightgreen", "chocolate3", "lightgoldenrodyellow", "lightsalmon")
boxplot(HR.Rest1~ Train.Effect.Cat, data = FourMileRun, outline = TRUE, col = colors,cex.axis = 0.8,main = "Box Plot of Heart Rate 1 Minute After Run  by Training Effect Category", xlab = "Training Effect Category", ylab = "Heart Rate 1 Minute After Run")
#------------------------------ MORE PLOTS ----------------------------------------------
#Hr difference between HR.Change2 and HR.Change1
HR.Diff <- FourMileRun$HR.Rest1 - FourMileRun$HR.Rest2
#hist(HR.Diff,main='Distribution of Heart Rate Difference Between 2 and one minute later',xlab='Heart Rate Difference',ylim=c(0,8),col="burlywood1")
# Create a boxplot of Heart Rate Difference Between 2 and 1 minute later in each Training Effect Category
colors <- c("skyblue", "lightgreen", "lightcoral", "lightgoldenrodyellow", "lightsalmon")
boxplot(HR.Diff ~ Train.Effect.Cat, data = FourMileRun, outline = TRUE, col = colors,cex.axis = 0.8,main = "Box Plot of Heart Rate Difference Between 2 and 1 minute later (Beats Per Minute) by Training Effect", xlab = "Training Effect Category", ylab = "HR.diff")
#Scatter plot of HR from start of rest and 1 minute later and
#and HR from the start of rest and 2 minutes later in each training effect category
plot(FourMileRun$HR.Rest1,FourMileRun$HR.Rest2,col=FourMileRun$Train.Effect.Cat,pch=as.numeric(FourMileRun$Train.Effect.Cat),xlab="Heart Rate 1 Minute AFter the Run",ylab = "Heart Rate 2 Minutes AFter the Run",main="Scatter Plot Of HR.Rest1 and HR.Rest2 by Training Effect Category",cex.main=0.8)
legend('bottomright', pch=1:5, col=1:5, legend=levels(FourMileRun$Train.Effect.Cat))
abline(lm(FourMileRun$HR.Rest2~FourMileRun$HR.Rest1))
#---------------------------------------- MODEL CONSTRUCTION --------------------------------------------------------
#Construct a regression model (or more) to assess the progress of Kevin.
#--------------------------- Model 1: All variables ----------------------------------------------------
progress.m1<- lm(Training.Effect ~ Calories.Burned +HR.Change1 +
HR.Change2 + Max.HR +Avg.HR + Avg.Speed + Max.Speed +
HR.Rest + HR.Rest1 + HR.Rest2 + Time.In.Minutes + Pace.In.Minutes , data = FourMileRun)
summary(progress.m1)
anova(progress.m1)
#from the anova we see that only HR.Change1, Max.HR and Avg.HR are statistically significant
# Residuals analysis
#(Unstandardized) Residuals
residuals(progress.m1)
#Studentized residuals
rstudent(progress.m1)
#Standardized residuals
rstandard(progress.m1)
res.progress.m1=residuals(progress.m1,type="pearson")
# Plot residuals vs. fitted values
plot(fitted(progress.m1),res.progress.m1)
#Normality QQ plot for (Unstandardized) Residuals
qqnorm(residuals(progress.m1))
qqline(residuals(progress.m1))
shapiro.test(residuals(progress.m1))
# we have normality for the unstardardized residuals
# Histogram and QQ plots for different residuals
par(mfcol = c(2, 3))
allres <- list(progress.m1$res, rstandard(progress.m1), rstudent(progress.m1))
mt <- c('Unstandardized Residuals', 'Standardized Residuals (Int. Stud.)', 'Studentized Residuals (Ext. Stud.)')
for (i in 1:3) {
x <- allres[[i]]
hist(x, probability = TRUE, main = mt[i], col = "lightgreen")
x0 <- seq(min(x), max(x), length.out = 100)
y0 <- dnorm(x0, mean(x), sd(x))
lines(x0, y0, col = 2, lty = 2)
qqnorm(x, main = mt[i])
qqline(x)
}
par(mfrow = c(1, 1))
qqPlot(rstudent(progress.m1))
# Independence check: time-sequence plot
par(mfrow = c(1, 2))
plot(progress.m1$res, type = 'l')
plot(rstandard(progress.m1), type = 'l')
runs.test(progress.m1$residuals)
# Homoscedasticity check
yhat <- fitted(progress.m1)
qyhat <- cut(yhat, breaks = 4)
leveneTest(rstandard(progress.m1) ~ qyhat)
# Non-linearity check
plot(progress.m1, which = 1)
#--------------------  Model 2: only with the Max.HR variable -----------------------------------
progress.m2 <- lm(Training.Effect~Max.HR  ,data = FourMileRun)
progress.m2.quadr <- lm(Training.Effect~Max.HR +I(Max.HR^2)  ,data = FourMileRun)
# Summarize the model
summary(progress.m2)
#Multiple R-squared:  0.3372 is relatively low
#we can see fromt the scatterplot weak linear fit
plot(FourMileRun$Max.HR, FourMileRun$Training.Effect, pch = 16, col = "forestgreen", xlab = "Max.HR", ylab = "Training.Effect", main = "Scatter Plot of Max.HR vs Training.Effect",cex.main = 0.9)
abline(lm(FourMileRun$Training.Effect~FourMileRun$Max.HR))
# Plot the quadratic curve
plot(Training.Effect ~ Max.HR, data = FourMileRun, main = "Quadratic Regression Plot", xlab = "Max.HR", ylab = "Training Effect")
# Add the quadratic curve
curve(predict(progress.m2.quadr, newdata = data.frame(Max.HR = x)), col = "red", lwd = 2, add = TRUE)
title("Quadratic Regression Plot")
legend("topleft", legend = "Quadratic Model", col = "red", lty = 1, lwd = 2)
#non linearity between Max.HR and training effect
#residuals vs. fitted values plot
plot(progress.m2, which=3)
plot(residuals(progress.m2))
abline(0, 0)
#p value =0.004887 so we reject the null Hypothesis so we do not have normality for residuals
shapiro.test(residuals(progress.m2))
#--------------------  Model 2: only with the Avg.HR variable -----------------------------------
progress.m3 <- lm(Training.Effect~Avg.HR  ,data = FourMileRun)
summary(progress.m3)
#Multiple R-squared:0.6764 is relatively large so there is a moderately strong relationship
# Scatter plot for Model 3
plot(FourMileRun$Avg.HR, FourMileRun$Training.Effect, pch = 16, col = "darkmagenta", xlab = "Avg.HR", ylab = "Training.Effect", main = "Scatter Plot of Avg.HR vs Training.Effect",cex.main = 0.9)
abline(lm(FourMileRun$Training.Effect~FourMileRun$Avg.HR))
#analysis of variance
anova(progress.m3)
#p value =0.0418 so we reject the null Hypothesis so we do not have normality for residuals
shapiro.test(residuals(progress.m3))
#Normality QQ plot for (Unstandardized) Residuals
qqnorm(residuals(progress.m3))
qqline(residuals(progress.m3))
par( mfcol=c(2,3) )
allres <- list(); allres[[1]] <- progress.m3$res
allres[[2]] <- rstandard(progress.m3)
allres[[3]] <- rstudent(progress.m3)
mt<-c(); mt[1] <- 'Unstandardized Residuals'
mt[2] <- 'Standardized Residuals (Int. Stud.)'
mt[3] <- 'Studentized Residuals (Ext. Stud.)'
for (i in 1:3){
x<-allres[[i]]
hist(x, probability=T, main=mt[i],col = "lightgreen")
x0<-seq( min(x), max(x), length.out=100)
y0<-dnorm( x0, mean(x), sd(x) )
lines(x0,y0, col=2, lty=2)
qqnorm(x, main=mt[i])
qqline(x)
}
par(mfrow = c(1, 1))
#For studentized residuals use the qq.plot function in car which compared them with the t distribution
qqPlot(rstudent(progress.m3))
#Checking for independence,Simple time‐sequence plot
par( mfrow=c(1,2) )
plot(progress.m3$res, type='l')
plot(rstandard(progress.m3), type='l')
runs.test(progress.m3$residuals)
# p-value = 0.4596 so we do not reject H0:the sequence was produced in a random manner, so we have independence
#non linearity between Avg.HR and training effect
#residuals vs. fitted values plot
plot(progress.m3, which=3)
plot(x = progress.m3$fitted.values, y = progress.m3$residuals,
xlab = "Fitted Values", ylab = "Residuals",
main = "Residuals vs Fitted Values")
# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red", lty = 2)
# Homoscedasticity check
yhat <- fitted(progress.m3)
qyhat <- cut(yhat, breaks = 4)
leveneTest(rstandard(progress.m3) ~ qyhat)
leveneTest( rstandard(progress.m3)~qyhat )
#pvalue=0.6413 so we do not reject Ho The variances of the residuals are equal across quartiles of the fitted values.
#no evidence to suggest a significant difference in variances among the groups defined by the quartiles of the fitted values. assumption of homoscedasticity may holds
#------------------------------ Model 4: Avg.HR with quadratic effect -----------------------------
#because we see non-linearity between Avg.HR and Training Effect we add the quadratic effect
progress.m4 <- lm(Training.Effect~Avg.HR + I(Avg.HR^2)  ,data = FourMileRun)
# Summarize the model
summary(progress.m4)
#model may fit the data well
#Multiple R-squared:  0.8893 is large so we may have a good model
#analysis of variance
#the model parameters are extremely statistically significant
anova(progress.m4)
#non linearity between Max.HR and training effect
plot(progress.m4, which=3)
#p value =0.5502 so we do not reject the null Hypothesis so we do have normality for residuals
shapiro.test(residuals(progress.m4))
#Normality QQ plot for (Unstandardized) Residuals
qqnorm(residuals(progress.m4))
qqline(residuals(progress.m4))
# Histogram and QQ plots for different residuals
par(mfcol = c(2, 3))
allres <- list(progress.m4$res, rstandard(progress.m4), rstudent(progress.m4))
mt <- c('Unstandardized Residuals', 'Standardized Residuals (Int. Stud.)', 'Studentized Residuals (Ext. Stud.)')
for (i in 1:3) {
x <- allres[[i]]
hist(x, probability = TRUE, main = mt[i], col = "lightgreen")
x0 <- seq(min(x), max(x), length.out = 100)
y0 <- dnorm(x0, mean(x), sd(x))
lines(x0, y0, col = 2, lty = 2)
qqnorm(x, main = mt[i])
qqline(x)
}
par(mfrow = c(1, 1))
#residuals vs. fitted values plot
#no linearity
plot(progress.m4, which=3)
#Checking for equality of variance in quartiles of fitted values
yhat <- fitted(progress.m4)
qyhat<- cut( yhat, breaks= 4 )
leveneTest( rstandard(progress.m4)~qyhat )
#pvalue=0.3165 so we do not reject Ho The variances of the residuals are equal
#across quartiles of the fitted values.
#no evidence to suggest a significant difference in variances among the groups
#defined by the quartiles of the fitted values. assumption of homoscedasticity may holds
#---------------------------------- Scatter plots and regression ---------------------------------------
par(mfrow = c(1, 2))
# Scatter Plot of Avg HR vs Training Effect
plot(FourMileRun$Avg.HR, FourMileRun$Training.Effect, pch = 16, col = "bisque3",
xlab = "Avg HR", ylab = "Training Effect", main = "Avg HR vs Training Effect",
cex.main = 1.5, cex.lab = 1.6, cex.axis = 1.6)
abline(lm(FourMileRun$Training.Effect ~ FourMileRun$Avg.HR))
# Quadratic Regression Plot
plot(Training.Effect ~ Avg.HR, data = FourMileRun, main = "Quadratic Regression Plot",
xlab = "Avg.HR", ylab = "Training Effect", cex.main = 1.5, cex.lab = 1.5, cex.axis = 1.5)
curve(predict(progress.m4, newdata = data.frame(Avg.HR = x)), col = "red", lwd = 2, add = TRUE)
legend("topleft", legend = "Quadratic Curve", col = "red", lty = 1, lwd = 2, cex = 1)
par(mfrow = c(1, 1))
# Add the quadratic curve
curve(predict(progress.m4, newdata = data.frame(Avg.HR = x)), col = "red", lwd = 2, add = TRUE)
# Add legend
legend("topleft", legend = "Quadratic Curve", col = "red", lty = 1, lwd = 2, cex = 1)
par(mfrow = c(1, 1))
#no clear pattern
# Residuals vs Fitted Values
plot(x = progress.m4$fitted.values, y = progress.m4$residuals,
xlab = "Fitted Values", ylab = "Residuals",
main = "Residuals vs Fitted Values")
abline(h = 0, col = "red", lty = 2)
#----------------- MODEL SELECTION AND CONSTRUCTION FROM THE BEGINING ------------------------
# Fit the model
subset_data <- FourMileRun[, !(names(FourMileRun) %in% c("Time", "Time.Minutes", "Pace.Minutes", "Pace.Seconds", "Time.In.Seconds", "Pace", "Pace.In.Seconds", "Time.Seconds","Run","Train.Effect.Cat","Pace.In.Minutes"))]
#My response variable is ordinal
# Fit the full model
full_model <- lm(Training.Effect ~ ., data = subset_data)
#statistically significant only the Max.HR,Avg.HR and HR.Rest in summary
summary(full_model)
anova(full_model)
# Residuals analysis for full model
plot(full_model, which=2)
#non linearity
plot(full_model, which=3)
#------------------------- STEPWISE FOR MODEL SELECTION ---------------------------------------
null_model <- lm(Training.Effect ~ 1, data = subset_data)
step(full_model, scope = list(lower = null_model, upper = full_model), direction = 'both')
# Bayesian Model Averaging with BIC and AIC priors
bas_results_bic <- bas.lm(Training.Effect ~ ., data = subset_data, prior = "BIC")
plot(bas_results_bic)
#image(bas_results_aic)
# Best model selection and evaluation
best_model <- lm(formula = Training.Effect ~ Max.HR + Avg.HR + HR.Rest + HR.Rest1 +
Time.In.Minutes, data = subset_data)
vif(best_model)
X <- model.matrix(best_model)
summary(best_model)
anova(best_model)
# Residuals analysis for best model
shapiro.test(residuals(best_model))  # Normality test
qqnorm(residuals(best_model))
qqline(residuals(best_model))
# Histograms and QQ plots for residuals
par(mfcol = c(2, 3))
allres <- list(best_model$res, rstandard(best_model), rstudent(best_model))
mt <- c('Unstandardized Residuals', 'Standardized Residuals (Int. Stud.)', 'Studentized Residuals (Ext. Stud.)')
for (i in 1:3) {
x <- allres[[i]]
hist(x, probability = TRUE, main = mt[i], col = "lightgreen")
x0 <- seq(min(x), max(x), length.out = 100)
y0 <- dnorm(x0, mean(x), sd(x))
lines(x0, y0, col = 2, lty = 2)
qqnorm(x, main = mt[i])
qqline(x)
}
par(mfrow = c(1, 1))
# Homoscedasticity check
yhat <- fitted(best_model)
qyhat <- cut(yhat, breaks = 4)
leveneTest(rstandard(best_model) ~ qyhat)
#pvalue=0.06913so we do not reject Ho The variances of the residuals are equal across quartiles of the fitted values.
#no evidence to suggest a significant difference in variances among the groups defined by the quartiles of the fitted values. assumption of homoscedasticity may holds
#---------------------------- MODEL WITH THE QUADRATIC EFFECT ---------------------------
best_model1 <- lm(formula = Training.Effect ~ Max.HR + Avg.HR + + I(Avg.HR^2) +HR.Rest + HR.Rest1 +
Time.In.Minutes, data = subset_data)
summary(best_model1)
anova(best_model1)
#we see that HR.Rest and HR.Rest1 are highly statistically insignificant
#------------------------- Model without HR.Rest and HR.Rest1 -------------------------------
best_model2<- lm(formula = Training.Effect ~ Max.HR + Avg.HR + I(Avg.HR^2) +
Time.In.Minutes , data = subset_data)
summary(best_model2)
anova(best_model2)
AIC(progress.m4,progress.m1,best_model,best_model1,best_model2)
#p value =0.09369 so we do not reject the null Hypothesis so we do have normality for residuals
shapiro.test(residuals(best_model2))
#Normality QQ plot for (Unstandardized) Residuals
qqnorm(residuals(best_model2))
qqline(residuals(best_model2))
par( mfcol=c(2,3) )
allres <- list(); allres[[1]] <- best_model2$res
allres[[2]] <- rstandard(best_model2)
allres[[3]] <- rstudent(best_model2)
mt<-c(); mt[1] <- 'Unstandardized Residuals'
mt[2] <- 'Standardized Residuals (Int. Stud.)'
mt[3] <- 'Studentized Residuals (Ext. Stud.)'
for (i in 1:3){
x<-allres[[i]]
hist(x, probability=T, main=mt[i],col = "lightgreen")
x0<-seq( min(x), max(x), length.out=100)
y0<-dnorm( x0, mean(x), sd(x) )
lines(x0,y0, col=2, lty=2)
qqnorm(x, main=mt[i])
qqline(x)
}
par(mfrow = c(1, 1))
# Homoscedasticity check for best_model2
yhat <- fitted(best_model2)
qyhat <- cut(yhat, breaks = 4)
leveneTest(rstandard(best_model2) ~ qyhat)
# Independence check
par(mfrow = c(1, 2))
plot(best_model2$res, type = 'l')
plot(rstandard(best_model2), type = 'l')
runs.test(best_model2$residuals)
# p-value = 0.627 so we do not reject H0:the sequence was produced in a random manner, so we have independence
par(mfrow = c(1, 1))
#no clear pattern
plot(x =best_model2$fitted.values, y = best_model2$residuals,
xlab = "Fitted Values", ylab = "Residuals",
main = "Residuals vs Fitted Values")
# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red", lty = 2)
#----------------------------- Best model 3 with quadratic effect -----------------------------
best_model3<- lm(formula = Training.Effect ~  Avg.HR + I(Avg.HR^2) +
Time.In.Minutes , data = subset_data)
summary(best_model3)
anova(best_model3)
model_coefficients <- coef(best_model3)
formatted_coefficients <- format(model_coefficients, scientific = FALSE)
print(formatted_coefficients)
#p value =0.3185 so we do not reject the null Hypothesis so we do have normality for residuals
shapiro.test(residuals(best_model3))
#Normality QQ plot for (Unstandardized) Residuals
qqnorm(residuals(best_model3))
qqline(residuals(best_model3))
par( mfcol=c(2,3) )
allres <- list(); allres[[1]] <- best_model3$res
allres[[2]] <- rstandard(best_model3)
allres[[3]] <- rstudent(best_model3)
mt<-c(); mt[1] <- 'Unstandardized Residuals'
mt[2] <- 'Standardized Residuals (Int. Stud.)'
mt[3] <- 'Studentized Residuals (Ext. Stud.)'
for (i in 1:3){
x<-allres[[i]]
hist(x, probability=T, main=mt[i],col = "lightgreen",cex.axis=2, cex.lab=2)
x0<-seq( min(x), max(x), length.out=100)
y0<-dnorm( x0, mean(x), sd(x) )
lines(x0,y0, col=2, lty=2)
qqnorm(x, main=mt[i], cex.axis=2, cex.lab=2)
qqline(x)
}
#Checking for equality of variance in quartiles of fitted values
yhat <- fitted(best_model3)
qyhat<- cut( yhat, breaks= 4 )
leveneTest( rstandard(best_model3)~qyhat )
#pvalue= 0.4155 so we do not reject Ho The variances of the residuals are equal across quartiles of the fitted values.
#no evidence to suggest a significant difference in variances among the groups defined by the quartiles of the fitted values. assumption of homoscedasticity may holds
#------------------------------ INDEPENDENCE PLOTS FOR best_model3-----------------------------------------------------
par(mfrow=c(1,2), cex.axis=1.8, cex.lab=1.8, cex.main=1.8)
# Plot unstandardized and standardized residuals
plot(best_model3$res, type = 'l', xlab = 'Index', ylab = 'Unstandardized Residuals', main = '')
plot(rstandard(best_model3), type = 'l', xlab = 'Index', ylab = 'Standardized Residuals', main = '')
runs.test(best_model3$residuals)
# p-value = 0.627 so we do not reject H0:the sequence was produced in a random manner, so we have independence
par(mfrow = c(1, 1), cex.axis=1.5, cex.lab=1.5, cex.main=1.5)
#no pattern
# Residuals vs Fitted Values
plot(x = best_model3$fitted.values, y = best_model3$residuals,
xlab = "Fitted Values", ylab = "Residuals", main = "", cex = 1.5)
abline(h = 0, col = "red", lty = 2)
# Additional plots for residuals
par( mfrow=c(1,2) )
plot(best_model3$res, type='l')
plot(rstandard(best_model3), type='l')
# Plotting residuals vs fitted values
plot(best_model3, which = 1)
### ----------------------------- PROCESS FOR CENTERING THE PREDICTORS OF OUR MODEL -------------------------------------
# Calculate the mean of the explan. variables
mean_Avg_HR <- mean(subset_data$Avg.HR)
mean_Time_In_Minutes <- mean(subset_data$Time.In.Minutes)
# Center the predictors by subtracting their mean values
subset_data$Centered_Avg_HR <- subset_data$Avg.HR - mean_Avg_HR
subset_data$Centered_Time_In_Minutes <- subset_data$Time.In.Minutes - mean_Time_In_Minutes
# Fit the lm with centered predictors
centered_model <- lm(Training.Effect ~ Centered_Avg_HR + I(Centered_Avg_HR^2) + Centered_Time_In_Minutes,
data = subset_data)
summary(centered_model)
#p value =0.3185 so we do not reject the null Hypothesis so we do have normality for residuals
shapiro.test(residuals(centered_model))
#Normality QQ plot for (Unstandardized) Residuals
qqnorm(residuals(centered_model))
qqline(residuals(centered_model))
# Histograms and QQ plots for residuals of centered_model
par(mfcol = c(2, 3))
allres <- list(centered_model$res, rstandard(centered_model), rstudent(centered_model))
mt <- c('Unstandardized Residuals', 'Standardized Residuals (Int. Stud.)', 'Studentized Residuals (Ext. Stud.)')
for (i in 1:3) {
x <- allres[[i]]
hist(x, probability = TRUE, main = mt[i], col = "lightgreen")
x0 <- seq(min(x), max(x), length.out = 100)
y0 <- dnorm(x0, mean(x), sd(x))
lines(x0, y0, col = 2, lty = 2)
qqnorm(x, main = mt[i])
qqline(x)
}
# Homoscedasticity check for centered_model
yhat <- fitted(centered_model)
qyhat <- cut(yhat, breaks = 4)
leveneTest(rstandard(centered_model) ~ qyhat)
#pvalue= 0.4155 so we do not reject Ho The variances of the residuals are equal across quartiles of the fitted values.
#no evidence to suggest a significant difference in variances among the groups defined by the quartiles of the fitted values. assumption of homoscedasticity may holds
# Independence check for centered_model
#Checking for independence,Simple time‐sequence plot
par( mfrow=c(1,2) )
plot(centered_model$res, type='l')
plot(rstandard(centered_model), type='l')
runs.test(centered_model$residuals)
# p-value = 0.627 so we do not reject H0:the sequence was produced in a random manner, so we have independence
par(mfrow = c(1, 1))
#no clear pattern
plot(x =centered_model$fitted.values, y = centered_model$residuals,
xlab = "Fitted Values", ylab = "Residuals",
main = "Residuals vs Fitted Values")
# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red", lty = 2)
par( mfrow=c(1,2) )
plot(centered_model$res, type='l')
plot(rstandard(centered_model), type='l')
# Plotting residuals vs fitted values
plot(centered_model, which = 1)
#---------------------------------  MODEL COMPARISON USING AIC -------------------------------------------------
AIC(progress.m4,progress.m1,best_model,best_model1,best_model2,best_model3,centered_model)
#Model Predictions
# Model Predictions
# Construct a hypothetical new data frame for predictions
new_data <- data.frame(
Avg.HR = c(132, 140, 145, 150, 154, 151, 137, 128, 135, 142, 147, 152),
Time.In.Minutes = c(34.3, 32.5, 33.0, 33.5, 33.23, 33.45, 34.05, 34.18, 32.8, 33.3, 34.0, 33.6)
)
new_data$Centered_Avg_HR <- new_data$Avg.HR - mean_Avg_HR
new_data$Centered_Time_In_Minutes <- new_data$Time.In.Minutes - mean_Time_In_Minutes
# Make predictions using the centered model
predictions <- predict(best_model3, newdata = new_data)
new_data$Predicted_Training_Effect <- predictions
subset_new_data <- new_data[, !(names(new_data) %in% c("Centered_Avg_HR", "Centered_Time_In_Minutes"))]
# Ordinal Logistic Regression
subset_data_ordinal_model <- FourMileRun[, !(names(FourMileRun) %in% c("Time", "Time.Minutes", "Pace.Minutes", "Pace.Seconds", "Time.In.Seconds", "Pace", "Pace.In.Seconds", "Time.Seconds", "Run", "Pace.In.Minutes", "Training.Effect"))]
full_model_ordinal <- polr(Train.Effect.Cat ~ ., data = subset_data_ordinal_model, Hess = TRUE)
null_model_ordinal <- polr(Train.Effect.Cat ~ 1, data = subset_data_ordinal_model, Hess = TRUE)
stepwise_model <- stepAIC(null_model_ordinal, scope = list(lower = null_model_ordinal, upper = full_model_ordinal), direction = "both")
# Summary of stepwise model
summary(stepwise_model)
exp(coef(stepwise_model))
#meaning the odds of the probability of Minor  when Avg.HR is at its reference level (here we will say the mean)
#assume that the mean is the reference level
#When you increase the average heart rate (Avg.HR) by one unit the odds of being in a higher training effect category increase by a factor of 1.20
#the odds increase by 20.6%
#higher average heart rates, are associated with more significant improvements in training effect categories.
# Perform Bayesian Model Averaging
bma_results <- bas.lm(Training.Effect ~ .,
data=subset_data, modelprior=uniform(), method="MCMC", MCMC.iterations=10000)
# Summary of BMA results
summary(bma_results)
par( mfrow=c(1,1) )
# Plotting residuals vs fitted values
plot(centered_model, which = 1)
#---------------------------------  MODEL COMPARISON USING AIC -------------------------------------------------
AIC(progress.m4,progress.m1,best_model,best_model1,best_model2,best_model3,centered_model)
#Model Predictions
# Model Predictions
# Construct a hypothetical new data frame for predictions
new_data <- data.frame(
Avg.HR = c(132, 140, 145, 150, 154, 151, 137, 128, 135, 142, 147, 152),
Time.In.Minutes = c(34.3, 32.5, 33.0, 33.5, 33.23, 33.45, 34.05, 34.18, 32.8, 33.3, 34.0, 33.6)
)
new_data$Centered_Avg_HR <- new_data$Avg.HR - mean_Avg_HR
new_data$Centered_Time_In_Minutes <- new_data$Time.In.Minutes - mean_Time_In_Minutes
# Make predictions using the centered model
predictions <- predict(best_model3, newdata = new_data)
new_data$Predicted_Training_Effect <- predictions
subset_new_data <- new_data[, !(names(new_data) %in% c("Centered_Avg_HR", "Centered_Time_In_Minutes"))]
# Ordinal Logistic Regression
subset_data_ordinal_model <- FourMileRun[, !(names(FourMileRun) %in% c("Time", "Time.Minutes", "Pace.Minutes", "Pace.Seconds", "Time.In.Seconds", "Pace", "Pace.In.Seconds", "Time.Seconds", "Run", "Pace.In.Minutes", "Training.Effect"))]
full_model_ordinal <- polr(Train.Effect.Cat ~ ., data = subset_data_ordinal_model, Hess = TRUE)
null_model_ordinal <- polr(Train.Effect.Cat ~ 1, data = subset_data_ordinal_model, Hess = TRUE)
stepwise_model <- stepAIC(null_model_ordinal, scope = list(lower = null_model_ordinal, upper = full_model_ordinal), direction = "both")
# Summary of stepwise model
summary(stepwise_model)
exp(coef(stepwise_model))
#meaning the odds of the probability of Minor  when Avg.HR is at its reference level (here we will say the mean)
#assume that the mean is the reference level
#When you increase the average heart rate (Avg.HR) by one unit the odds of being in a higher training effect category increase by a factor of 1.20
#the odds increase by 20.6%
#higher average heart rates, are associated with more significant improvements in training effect categories.
# Perform Bayesian Model Averaging
bma_results <- bas.lm(Training.Effect ~ .,
data=subset_data, modelprior=uniform(), method="MCMC", MCMC.iterations=10000)
# Summary of BMA results
summary(bma_results)
